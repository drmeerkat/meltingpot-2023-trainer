nohup: ignoring input

=============
== PyTorch ==
=============

NVIDIA Release 23.08 (build 66128610)
PyTorch Version 2.1.0a0+29c30b1

Container image Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

Copyright (c) 2014-2023 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

WARNING: CUDA Minor Version Compatibility mode ENABLED.
  Using driver version 525.147.05 which has support for CUDA 12.0.  This container
  was built with CUDA 12.2 and will be run in Minor Version Compatibility mode.
  CUDA Forward Compatibility is preferred over Minor Version Compatibility for use
  with this container but was unavailable:
  [[Forward compatibility was attempted on non supported HW (CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE) cuInit()=804]]
  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.

pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)
Hello from the pygame community. https://www.pygame.org/contribute.html
/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = _posixsubprocess.fork_exec(
2024-04-16 04:45:58,135	INFO worker.py:1752 -- Started a local Ray instance.
2024-04-16 04:45:59,025	INFO tune.py:622 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:130: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  gym.logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64[0m
  logger.warn(
/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.[0m
  logger.warn(f"{pre} is not within the observation space.")
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Configuration for experiment     al_harvest_base       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Search algorithm                 BasicVariantGenerator â”‚
â”‚ Scheduler                        FIFOScheduler         â”‚
â”‚ Number of trials                 1                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

View detailed results here: /workspace/logs/20240416-0445-57-ray-logs/torch/al_harvest_base
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-04-16_04-45-57_582158_1/artifacts/2024-04-16_04-45-59/al_harvest_base/driver_artifacts`
[36m(RolloutWorker pid=2246)[0m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=2246, ip=172.21.0.4, actor_id=834a4f1a064f39dbd25fba2a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa22134d8a0>)
[36m(RolloutWorker pid=2246)[0m   File "/workspace/code/environments/al_harvest_env.py", line 90, in step
[36m(RolloutWorker pid=2246)[0m     if self.use_custom_reward:
[36m(RolloutWorker pid=2246)[0m AttributeError: 'AlHarvestMeltingPotEnv' object has no attribute 'use_custom_reward'
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m The above exception was the direct cause of the following exception:
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m [36mray::RolloutWorker.__init__()[39m (pid=2246, ip=172.21.0.4, actor_id=834a4f1a064f39dbd25fba2a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa22134d8a0>)
[36m(RolloutWorker pid=2246)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
[36m(RolloutWorker pid=2246)[0m     check_multiagent_environments(env)
[36m(RolloutWorker pid=2246)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
[36m(RolloutWorker pid=2246)[0m     raise ValueError(
[36m(RolloutWorker pid=2246)[0m ValueError: Your environment (<AlHarvestMeltingPotEnv instance>) does not abide to the new gymnasium-style API!
[36m(RolloutWorker pid=2246)[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
[36m(RolloutWorker pid=2246)[0m In particular, the `step()` method seems to be faulty.
[36m(RolloutWorker pid=2246)[0m Learn more about the most important changes here:
[36m(RolloutWorker pid=2246)[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m In order to fix this problem, do the following:
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m 1) Run `pip install gymnasium` on your command line.
[36m(RolloutWorker pid=2246)[0m 2) Change all your import statements in your code from
[36m(RolloutWorker pid=2246)[0m    `import gym` -> `import gymnasium as gym` OR
[36m(RolloutWorker pid=2246)[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m For your custom (single agent) gym.Env classes:
[36m(RolloutWorker pid=2246)[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
[36m(RolloutWorker pid=2246)[0m      EnvCompatibility` wrapper class.
[36m(RolloutWorker pid=2246)[0m 3.2) Alternatively to 3.1:
[36m(RolloutWorker pid=2246)[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,
[36m(RolloutWorker pid=2246)[0m    seed=None, options=None)'
[36m(RolloutWorker pid=2246)[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`
[36m(RolloutWorker pid=2246)[0m    method.
[36m(RolloutWorker pid=2246)[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and
[36m(RolloutWorker pid=2246)[0m    `info`). This flag should indicate, whether the episode was terminated prematurely
[36m(RolloutWorker pid=2246)[0m    due to some time constraint or other kind of horizon setting.
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m For your custom RLlib `MultiAgentEnv` classes:
[36m(RolloutWorker pid=2246)[0m 4.1) Either wrap your old MultiAgentEnv via the provided
[36m(RolloutWorker pid=2246)[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
[36m(RolloutWorker pid=2246)[0m      MultiAgentEnvCompatibility` wrapper class.
[36m(RolloutWorker pid=2246)[0m 4.2) Alternatively to 4.1:
[36m(RolloutWorker pid=2246)[0m  - Change your `reset()` method to have the call signature
[36m(RolloutWorker pid=2246)[0m    'def reset(self, *, seed=None, options=None)'
[36m(RolloutWorker pid=2246)[0m  - Return an additional per-agent info dict (empty dict should be fine) from your
[36m(RolloutWorker pid=2246)[0m    `reset()` method.
[36m(RolloutWorker pid=2246)[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really
[36m(RolloutWorker pid=2246)[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit
[36m(RolloutWorker pid=2246)[0m    setting).
[36m(RolloutWorker pid=2246)[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`
[36m(RolloutWorker pid=2246)[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`
[36m(RolloutWorker pid=2246)[0m    per-agent dict).
[36m(RolloutWorker pid=2246)[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
[36m(RolloutWorker pid=2246)[0m    flag should indicate, whether the episode (for some agent or all agents) was
[36m(RolloutWorker pid=2246)[0m    terminated prematurely due to some time constraint or other kind of horizon setting.
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m During handling of the above exception, another exception occurred:
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m [36mray::RolloutWorker.__init__()[39m (pid=2246, ip=172.21.0.4, actor_id=834a4f1a064f39dbd25fba2a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa22134d8a0>)
[36m(RolloutWorker pid=2246)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 414, in __init__
[36m(RolloutWorker pid=2246)[0m     check_env(self.env, self.config)
[36m(RolloutWorker pid=2246)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 96, in check_env
[36m(RolloutWorker pid=2246)[0m     raise ValueError(
[36m(RolloutWorker pid=2246)[0m ValueError: Traceback (most recent call last):
[36m(RolloutWorker pid=2246)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 363, in check_multiagent_environments
[36m(RolloutWorker pid=2246)[0m     results = env.step(sampled_action)
[36m(RolloutWorker pid=2246)[0m   File "/workspace/code/environments/al_harvest_env.py", line 90, in step
[36m(RolloutWorker pid=2246)[0m     if self.use_custom_reward:
[36m(RolloutWorker pid=2246)[0m AttributeError: 'AlHarvestMeltingPotEnv' object has no attribute 'use_custom_reward'
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m The above exception was the direct cause of the following exception:
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m [36mray::RolloutWorker.__init__()[39m (pid=2246, ip=172.21.0.4, actor_id=834a4f1a064f39dbd25fba2a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa22134d8a0>)
[36m(RolloutWorker pid=2246)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
[36m(RolloutWorker pid=2246)[0m     check_multiagent_environments(env)
[36m(RolloutWorker pid=2246)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
[36m(RolloutWorker pid=2246)[0m     raise ValueError(
[36m(RolloutWorker pid=2246)[0m ValueError: Your environment (<AlHarvestMeltingPotEnv instance>) does not abide to the new gymnasium-style API!
[36m(RolloutWorker pid=2246)[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
[36m(RolloutWorker pid=2246)[0m In particular, the `step()` method seems to be faulty.
[36m(RolloutWorker pid=2246)[0m Learn more about the most important changes here:
[36m(RolloutWorker pid=2246)[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m In order to fix this problem, do the following:
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m 1) Run `pip install gymnasium` on your command line.
[36m(RolloutWorker pid=2246)[0m 2) Change all your import statements in your code from
[36m(RolloutWorker pid=2246)[0m    `import gym` -> `import gymnasium as gym` OR
[36m(RolloutWorker pid=2246)[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m For your custom (single agent) gym.Env classes:
[36m(RolloutWorker pid=2246)[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
[36m(RolloutWorker pid=2246)[0m      EnvCompatibility` wrapper class.
[36m(RolloutWorker pid=2246)[0m 3.2) Alternatively to 3.1:
[36m(RolloutWorker pid=2246)[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,
[36m(RolloutWorker pid=2246)[0m    seed=None, options=None)'
[36m(RolloutWorker pid=2246)[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`
[36m(RolloutWorker pid=2246)[0m    method.
[36m(RolloutWorker pid=2246)[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and
[36m(RolloutWorker pid=2246)[0m    `info`). This flag should indicate, whether the episode was terminated prematurely
[36m(RolloutWorker pid=2246)[0m    due to some time constraint or other kind of horizon setting.
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m For your custom RLlib `MultiAgentEnv` classes:
[36m(RolloutWorker pid=2246)[0m 4.1) Either wrap your old MultiAgentEnv via the provided
[36m(RolloutWorker pid=2246)[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
[36m(RolloutWorker pid=2246)[0m      MultiAgentEnvCompatibility` wrapper class.
[36m(RolloutWorker pid=2246)[0m 4.2) Alternatively to 4.1:
[36m(RolloutWorker pid=2246)[0m  - Change your `reset()` method to have the call signature
[36m(RolloutWorker pid=2246)[0m    'def reset(self, *, seed=None, options=None)'
[36m(RolloutWorker pid=2246)[0m  - Return an additional per-agent info dict (empty dict should be fine) from your
[36m(RolloutWorker pid=2246)[0m    `reset()` method.
[36m(RolloutWorker pid=2246)[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really
[36m(RolloutWorker pid=2246)[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit
[36m(RolloutWorker pid=2246)[0m    setting).
[36m(RolloutWorker pid=2246)[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`
[36m(RolloutWorker pid=2246)[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`
[36m(RolloutWorker pid=2246)[0m    per-agent dict).
[36m(RolloutWorker pid=2246)[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
[36m(RolloutWorker pid=2246)[0m    flag should indicate, whether the episode (for some agent or all agents) was
[36m(RolloutWorker pid=2246)[0m    terminated prematurely due to some time constraint or other kind of horizon setting.
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m 
[36m(RolloutWorker pid=2246)[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).
2024-04-16 04:46:06,965	ERROR tune_controller.py:1332 -- Trial task failed for trial PPO_meltingpot_382cf_00000
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
  File "/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
  File "python/ray/_raylet.pyx", line 2281, in ray._raylet.task_execution_handler
  File "python/ray/_raylet.pyx", line 2177, in ray._raylet.execute_task_with_cancellation_handler
  File "python/ray/_raylet.pyx", line 1832, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1833, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 2071, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1100, in ray._raylet.store_task_errors
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::PPO.__init__()[39m (pid=2137, ip=172.21.0.4, actor_id=e1f98d138f617b4c0789b65e01000000, repr=PPO)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py", line 533, in __init__
    super().__init__(
  File "/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py", line 161, in __init__
    self.setup(copy.deepcopy(self.config))
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py", line 631, in setup
    self.workers = WorkerSet(
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py", line 181, in __init__
    raise e.args[0].args[2]
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 414, in __init__
    check_env(self.env, self.config)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 96, in check_env
    raise ValueError(
ValueError: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 363, in check_multiagent_environments
    results = env.step(sampled_action)
  File "/workspace/code/environments/al_harvest_env.py", line 90, in step
    if self.use_custom_reward:
AttributeError: 'AlHarvestMeltingPotEnv' object has no attribute 'use_custom_reward'

The above exception was the direct cause of the following exception:

[36mray::PPO.__init__()[39m (pid=2137, ip=172.21.0.4, actor_id=e1f98d138f617b4c0789b65e01000000, repr=PPO)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
    raise ValueError(
ValueError: Your environment (<AlHarvestMeltingPotEnv instance>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
In particular, the `step()` method seems to be faulty.
Learn more about the most important changes here:
https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium

In order to fix this problem, do the following:

1) Run `pip install gymnasium` on your command line.
2) Change all your import statements in your code from
   `import gym` -> `import gymnasium as gym` OR
   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`

For your custom (single agent) gym.Env classes:
3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
     EnvCompatibility` wrapper class.
3.2) Alternatively to 3.1:
 - Change your `reset()` method to have the call signature 'def reset(self, *,
   seed=None, options=None)'
 - Return an additional info dict (empty dict should be fine) from your `reset()`
   method.
 - Return an additional `truncated` flag from your `step()` method (between `done` and
   `info`). This flag should indicate, whether the episode was terminated prematurely
   due to some time constraint or other kind of horizon setting.

For your custom RLlib `MultiAgentEnv` classes:
4.1) Either wrap your old MultiAgentEnv via the provided
     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
     MultiAgentEnvCompatibility` wrapper class.
4.2) Alternatively to 4.1:
 - Change your `reset()` method to have the call signature
   'def reset(self, *, seed=None, options=None)'
 - Return an additional per-agent info dict (empty dict should be fine) from your
   `reset()` method.
 - Rename `dones` into `terminateds` and only set this to True, if the episode is really
   done (as opposed to has been terminated prematurely due to some horizon/time-limit
   setting).
 - Return an additional `truncateds` per-agent dictionary flag from your `step()`
   method, including the `__all__` key (100% analogous to your `dones/terminateds`
   per-agent dict).
   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
   flag should indicate, whether the episode (for some agent or all agents) was
   terminated prematurely due to some time constraint or other kind of horizon setting.


The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).

Trial PPO_meltingpot_382cf_00000 errored after 0 iterations at 2024-04-16 04:46:06. Total running time: 7s
Error file: /tmp/ray/session_2024-04-16_04-45-57_582158_1/artifacts/2024-04-16_04-45-59/al_harvest_base/driver_artifacts/PPO_meltingpot_382cf_00000_0_2024-04-16_04-45-59/error.txt
2024-04-16 04:46:06,975	INFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/workspace/logs/20240416-0445-57-ray-logs/torch/al_harvest_base' in 0.0016s.

2024-04-16 04:46:06,976	ERROR tune.py:1044 -- Trials did not complete: [PPO_meltingpot_382cf_00000]
2024-04-16 04:46:06,977	WARNING experiment_analysis.py:190 -- Failed to fetch metrics for 1 trial(s):
- PPO_meltingpot_382cf_00000: FileNotFoundError('Could not fetch metrics for PPO_meltingpot_382cf_00000: both result.json and progress.csv were not found at /workspace/logs/20240416-0445-57-ray-logs/torch/al_harvest_base/PPO_meltingpot_382cf_00000_0_2024-04-16_04-45-59')
Result(
  error='RayActorError',
  metrics={},
  path='/workspace/logs/20240416-0445-57-ray-logs/torch/al_harvest_base/PPO_meltingpot_382cf_00000_0_2024-04-16_04-45-59',
  filesystem='local',
  checkpoint=None
)
[36m(RolloutWorker pid=2248)[0m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=2248, ip=172.21.0.4, actor_id=beb7d2ee7004a42f50a62e6601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f6214ab97e0>)[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[36m(RolloutWorker pid=2248)[0m   File "/workspace/code/environments/al_harvest_env.py", line 90, in step[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m     if self.use_custom_reward:[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m AttributeError: 'AlHarvestMeltingPotEnv' object has no attribute 'use_custom_reward'[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m [32m [repeated 34x across cluster][0m
[36m(RolloutWorker pid=2248)[0m The above exception was the direct cause of the following exception:[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m [36mray::RolloutWorker.__init__()[39m (pid=2248, ip=172.21.0.4, actor_id=beb7d2ee7004a42f50a62e6601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f6214ab97e0>)[32m [repeated 6x across cluster][0m
[36m(RolloutWorker pid=2248)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env[32m [repeated 6x across cluster][0m
[36m(RolloutWorker pid=2248)[0m     check_multiagent_environments(env)[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments[32m [repeated 6x across cluster][0m
[36m(RolloutWorker pid=2248)[0m     raise ValueError([32m [repeated 6x across cluster][0m
[36m(RolloutWorker pid=2248)[0m ValueError: Your environment (<AlHarvestMeltingPotEnv instance>) does not abide to the new gymnasium-style API![32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m In particular, the `step()` method seems to be faulty.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m Learn more about the most important changes here:[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m In order to fix this problem, do the following:[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m 1) Run `pip install gymnasium` on your command line.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m 2) Change all your import statements in your code from[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    `import gym` -> `import gymnasium as gym` OR[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m For your custom (single agent) gym.Env classes:[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m      EnvCompatibility` wrapper class.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m 4.2) Alternatively to 4.1:[32m [repeated 8x across cluster][0m
[36m(RolloutWorker pid=2248)[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    seed=None, options=None)'[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    method.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    `info`). This flag should indicate, whether the episode was terminated prematurely[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    due to some time constraint or other kind of horizon setting.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m For your custom RLlib `MultiAgentEnv` classes:[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m 4.1) Either wrap your old MultiAgentEnv via the provided[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m      MultiAgentEnvCompatibility` wrapper class.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m  - Change your `reset()` method to have the call signature[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    'def reset(self, *, seed=None, options=None)'[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m  - Return an additional per-agent info dict (empty dict should be fine) from your[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    `reset()` method.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    setting).[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    per-agent dict).[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    flag should indicate, whether the episode (for some agent or all agents) was[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m    terminated prematurely due to some time constraint or other kind of horizon setting.[32m [repeated 4x across cluster][0m
[36m(RolloutWorker pid=2248)[0m During handling of the above exception, another exception occurred:[32m [repeated 2x across cluster][0m
[36m(RolloutWorker pid=2248)[0m   File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 414, in __init__[32m [repeated 2x across cluster][0m
[36m(RolloutWorker pid=2248)[0m     check_env(self.env, self.config)[32m [repeated 2x across cluster][0m
[36m(RolloutWorker pid=2248)[0m ValueError: Traceback (most recent call last):[32m [repeated 2x across cluster][0m
[36m(RolloutWorker pid=2248)[0m     results = env.step(sampled_action)[32m [repeated 2x across cluster][0m
[36m(RolloutWorker pid=2248)[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).[32m [repeated 2x across cluster][0m
